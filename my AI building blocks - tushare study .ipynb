{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习使用Tushare， 下载A股数据\n",
    "\n",
    "参考：\n",
    "- [1]: https://mp.weixin.qq.com/s?__biz=MzAwOTgzMDk5Ng==&mid=2650833972&idx=1&sn=4de9f9ee81bc8bf85d1e0a4a8f79b0de&chksm=80adb30fb7da3a19817c72ff6f715ee91d6e342eb0402e860e171993bb0293bc4097e2dc4fe9&mpshare=1&scene=1&srcid=1106BPAdPiPCnj6m2Xyt5p2M#wechat_redirect\n",
    "- [2]: http://tushare.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\docul\\\\ai4stocks-projects\\\\myStockAILib')\n",
    "import stockbasic as sb\n",
    "import dataprep as dp\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import tushare as ts\n",
    "print(ts.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子，由沪深300 成分股，生成训练集\n",
    "\n",
    "#### Fetch full data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stock_list_hs300 = ts.get_hs300s()  # 沪深300成分股\n",
    "# drop '002839'\n",
    "\n",
    "sd = '2010-01-01'  # start date\n",
    "ed = '2017-12-01'  # end date\n",
    "\n",
    "# fetch all data in the list\n",
    "all_data = dp.fetch_raw_data(stock_list_hs300, sd, ed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv('hs300_20100101-20171124.csv')\n",
    "print(all_data.shape)\n",
    "print(all_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = pd.read_csv('hs300_20100101-20171124.csv', index_col=[1], dtype={'code':str})\n",
    "\n",
    "# set index\n",
    "rb.set_index([rb['code'], rb.index], inplace=True)\n",
    "# remove duplicated 'code.1'\n",
    "rb.drop(['code.1'], axis=1, inplace=True)\n",
    "\n",
    "print(rb.head(5))\n",
    "all_data = rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_list = all_data.groupby(level=0).size().reset_index(name='counts')\n",
    "for index, row in stock_list.iterrows():\n",
    "    print(row['code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Stock Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: all_data, MultiIndex'ed by 'code' and 'date'\n",
    "#\n",
    "\n",
    "CONST_DROP_THRESHOLD = 1000\n",
    "\n",
    "all_data_and_features = pd.DataFrame()  # stock data with all available features, such as 'SQZ', 'HIST1', and all.\n",
    "\n",
    "\n",
    "stock_list = all_data.groupby(level=0).size().reset_index(name='counts')\n",
    "# Initial call to print 0% progress\n",
    "total = stock_list.shape[0]\n",
    "i = 0\n",
    "dp.printProgressBar(i, total, prefix = 'Progress:', suffix = 'Complete', length = 60)\n",
    "\n",
    "\n",
    "for index, row in stock_list.iterrows():  # iterate the stock list\n",
    "    # slicing one stock_data\n",
    "    stock_data = all_data.loc[row['code']]\n",
    "    \n",
    "    # skip if stock_data has less than CONST_DROP_THRESHOLD bars\n",
    "    if stock_data.shape[0] < CONST_DROP_THRESHOLD:\n",
    "        i += 1\n",
    "        print('DROP ' + row['code'])\n",
    "        continue\n",
    "        \n",
    "    # print(row['code'], row['name'])\n",
    "    # print(stock_data.shape)\n",
    "    \n",
    "    # add 'EMA8', 'EMA21'\n",
    "    stock_data = stock_data.join(sb.ttm_propulsion(stock_data))\n",
    "    # add SQZ\n",
    "    stock_data = stock_data.join(sb.ttm_squeeze(stock_data))\n",
    "    # add WAVE\n",
    "    stock_data = stock_data.join(sb.ttm_wave(stock_data))\n",
    "    # add ADX\n",
    "    stock_data = stock_data.join(sb.talib_adx(stock_data))\n",
    "    # add ATR\n",
    "    stock_data = stock_data.join(sb.talib_atr(stock_data))\n",
    "    # add N-bar LOW\n",
    "    stock_data = stock_data.join(sb.talib_nbarlow(stock_data, N_BAR_LOWEST = 10))\n",
    "    \n",
    "    # append to all_data_and_features\n",
    "    all_data_and_features = all_data_and_features.append(stock_data)\n",
    "    \n",
    "    # update progress bar\n",
    "    i += 1\n",
    "    dp.printProgressBar(i, total, prefix = 'Progress:', suffix = 'Complete', length = 60)\n",
    "\n",
    "# MulitIndex with both 'code' and 'date'\n",
    "all_data_and_features.set_index([all_data_and_features['code'], all_data_and_features.index], inplace=True)\n",
    "\n",
    "print('original all_data: ')\n",
    "print(all_data.columns, '\\n', all_data.shape)\n",
    "\n",
    "print('after adding featuresa: ')\n",
    "print(all_data_and_features.columns, '\\n', all_data_and_features.shape)\n",
    "print(all_data_and_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to calculate how many stocks we have in all_data_and_features DataFrame\n",
    "all_data_and_features.groupby(level=0).size().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize X_all and Y_all\n",
    "X_all = pd.DataFrame()\n",
    "Y_all = pd.DataFrame()\n",
    "\n",
    "# Initial call to print 0% progress\n",
    "total = all_data_and_features.groupby(level=0).size().shape[0]\n",
    "i = 0\n",
    "dp.printProgressBar(i, total, prefix = 'Progress:', suffix = 'Complete', length = 60)\n",
    "\n",
    "for code, onestock_df in all_data_and_features.groupby(level=0):\n",
    "    print(code, ' ', onestock_df.shape)\n",
    "    onestock_X_all, onestock_Y_all = dp.generate_samples(onestock_df)\n",
    "    print(onestock_X_all.shape, onestock_Y_all.shape)\n",
    "    \n",
    "    # Add samples to X_all\n",
    "    X_all = X_all.append(onestock_X_all, ignore_index = False)\n",
    "    # Add sell-point information to Y_all.\n",
    "    Y_all = Y_all.append(onestock_Y_all, ignore_index = True)\n",
    "    \n",
    "    # update progress\n",
    "    i += 1\n",
    "    dp.printProgressBar(i, total, prefix = 'Progress:', suffix = 'Complete', length = 60)\n",
    "    \n",
    "    # testing\n",
    "    # if i > 5:\n",
    "    #    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Samples: ', Y_all.shape[0])\n",
    "print('Total timesteps in all Samples: ', X_all.shape[0])\n",
    "\n",
    "# check how many Samples have positive profit\n",
    "print('No. of samples with positive profit higher than 20%: ', Y_all[Y_all.sell_price > (Y_all.buy_price*1.2)]['code'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save X_all/Y_all (samples) to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all.to_csv('hs300_20100101-20171124-samples-X-all.csv')\n",
    "Y_all.to_csv('hs300_20100101-20171124-samples-Y-all.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read X_all/Yall from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read X_all samples\n",
    "rbx = pd.read_csv('hs300_20100101-20171124-samples-X-all.csv', index_col=[1], dtype={'code':str})\n",
    "\n",
    "# set index\n",
    "rbx.set_index([rbx['code'], rbx.index], inplace=True)\n",
    "# remove duplicated 'code.1'\n",
    "rbx.drop(['code.1'], axis=1, inplace=True)\n",
    "\n",
    "print(rbx.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Y_all samples\n",
    "rby = pd.read_csv('hs300_20100101-20171124-samples-Y-all.csv', index_col=[0], dtype={'code':str})\n",
    "\n",
    "print(rby.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update Y_all buy_date and sell_date\n",
    "\n",
    "buy_dates = []\n",
    "sell_dates = []\n",
    "\n",
    "for i in rby.index:\n",
    "    b_date = rby.iloc[i].buy_date[12:22]  # truncate string\n",
    "    buy_dates.append(b_date)\n",
    "    s_date = rby.iloc[i].sell_date[12:22]  # truncate string\n",
    "    sell_dates.append(s_date)\n",
    "    \n",
    "rby['buy_date'] = buy_dates\n",
    "rby['sell_date'] = sell_dates\n",
    "\n",
    "print(rby.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = rbx\n",
    "Y_all = rby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import tushare as ts\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.finance as mpf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(X_all.columns)\n",
    "print(Y_all.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given 'code', show stock's 'name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define stock plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import date2num\n",
    "import datetime\n",
    "\n",
    "#\n",
    "# date_to_num\n",
    "#\n",
    "\n",
    "\n",
    "def date_to_num(dates):\n",
    "    ''' Function to convert tushare 'date' string to matplotlib datenum\n",
    "    \n",
    "    Input\n",
    "    =====\n",
    "    dates: ndarray of tushare 'date' strings. Eg. ['2013-01-31', ...]\n",
    "    \n",
    "    Output\n",
    "    ======\n",
    "    Return: list of float datetime value compatible to matplotlib: floating point \n",
    "            numbers which represent time in days since 0001-01-01 UTC, plus 1. \n",
    "            For example, 0001-01-01, 06:00 is 1.25, not 0.25.\n",
    "    \n",
    "    Example\n",
    "    =======\n",
    "        stock_data['mpl.date'] = date_to_num(stock_data['date'].values)\n",
    "\n",
    "    '''\n",
    "    num_time = []\n",
    "    for date in dates:\n",
    "        date_time = datetime.datetime.strptime(date,'%Y-%m-%d')\n",
    "        num_date = date2num(date_time)\n",
    "        '''\n",
    "            matplotlib.dates.date2num(d)\n",
    "                Converts datetime objects to Matplotlib dates\n",
    "        '''\n",
    "        num_time.append(num_date)\n",
    "    return num_time\n",
    "\n",
    "\n",
    "#\n",
    "# plog_stock_data\n",
    "#\n",
    "\n",
    "\n",
    "def plot_stock_data(stock_data, title_postfix=''):\n",
    "    ''' Function to plot stock_data\n",
    "    \n",
    "    Input\n",
    "    =====\n",
    "    stock_data: DataFrame, with columns 'date', 'code', 'close', 'volumn', etc.\n",
    "    \n",
    "    Output\n",
    "    ======\n",
    "    Return: None\n",
    "    '''\n",
    "   \n",
    "    # make a local copy\n",
    "    sdata = stock_data.copy(deep=False)\n",
    "    # convert index 'date' to a column\n",
    "    sdata.reset_index(level=1, inplace=True)\n",
    "    \n",
    "    # convert date to num\n",
    "    sdata['mpl.date'] = date_to_num(sdata['date'].values)\n",
    "\n",
    "    fig, axes = plt.subplots(7, sharex=True, figsize=(15,14),\n",
    "                             gridspec_kw={'height_ratios':[3,1,1,1,1,1,1]})\n",
    "    \n",
    "    # axes[0]: k-line\n",
    "    mpf.candlestick_ochl(axes[0],\n",
    "                         sdata[['mpl.date', 'open', 'close', 'high', 'low']].values,\n",
    "                         width=1.0,\n",
    "                         colorup = 'g',\n",
    "                         colordown = 'r')\n",
    "    # axes[0]: EMA8, EMA21\n",
    "    axes[0].plot(sdata['mpl.date'].values, sdata['EMA8'].values, 'm', label='EMA8')\n",
    "    axes[0].plot(sdata['mpl.date'].values, sdata['EMA21'].values, 'c', label='EMA21')\n",
    "    axes[0].legend(loc=0)\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    axes[0].set_title(stock_data['code'].iloc[0] + ' ' + title_postfix)\n",
    "    axes[0].set_ylabel('Price')\n",
    "    axes[0].grid(True)\n",
    "    axes[0].xaxis_date()\n",
    "\n",
    "    # axes[1]: volume\n",
    "    axes[1].bar(sdata['mpl.date'].values-0.25, sdata['volume'].values, width= 0.5)\n",
    "    axes[1].set_ylabel('Volume')\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # axes[2]: MTMMA\n",
    "    bars = axes[2].bar(sdata['mpl.date'].values-0.25, sdata['MTMMA'].values, width=0.8)\n",
    "    for bar in bars:\n",
    "        if bar.get_height() > 0:\n",
    "            bar.set_color('g')\n",
    "        else:\n",
    "            bar.set_color('r')\n",
    "        \n",
    "    # axes[2]: SQUEEZE\n",
    "    patches = axes[2].plot(sdata['mpl.date'].values-0.25,\n",
    "                 [0 if x == sb.CONST_SQUEEZE_ONGOING else float('nan') for x in sdata['SQUEEZE'].values], # rescale\n",
    "                 'ko',\n",
    "                 label='SQUEEZE')\n",
    "    axes[2].set_ylabel('SQZ')\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    # axes[3]: TTM WAVE C\n",
    "    bars = axes[3].bar(sdata['mpl.date'].values-0.25, sdata['MACD6'].values, color='red', width=0.8, alpha=0.8)\n",
    "    bars = axes[3].bar(sdata['mpl.date'].values-0.25, sdata['HIST5'].values, color='orange', width=0.8, alpha=0.8)\n",
    "    axes[3].set_ylabel('WAVE C')\n",
    "    axes[3].grid(True)\n",
    "   \n",
    "    # axes[4]: TTM WAVE B\n",
    "    bars = axes[4].bar(sdata['mpl.date'].values-0.25, sdata['HIST4'].values, color='magenta', width=0.8, alpha=0.8)\n",
    "    bars = axes[4].bar(sdata['mpl.date'].values-0.25, sdata['HIST3'].values, color='teal', width=0.8, alpha=0.8)\n",
    "    axes[4].set_ylabel('WAVE B')\n",
    "    axes[4].grid(True)\n",
    "   \n",
    "    # axes[5]: TTM WAVE A\n",
    "    bars = axes[5].bar(sdata['mpl.date'].values-0.25, sdata['HIST2'].values, color='lawngreen', width=0.8, alpha=0.8)\n",
    "    bars = axes[5].bar(sdata['mpl.date'].values-0.25, sdata['HIST1'].values, color='yellow', width=0.8, alpha=0.8)\n",
    "    axes[5].set_ylabel('WAVE A')\n",
    "    axes[5].grid(True)\n",
    "    \n",
    "    # axes[6]: ADX\n",
    "    axes[6].plot(sdata['mpl.date'].values, sdata['ADX'].values, 'm', label='ADX')\n",
    "    axes[6].set_ylabel('ADX')\n",
    "    axes[6].grid(True)\n",
    "   \n",
    "    return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize one stock sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math\n",
    "\n",
    "# plot a random good sample\n",
    "\n",
    "CONST_PROFIT_THRESHOLD = 2  # profit bigger than\n",
    "\n",
    "while (1):\n",
    "    # generate a random number\n",
    "    pindex = random.randint(0, Y_all.shape[0]-1)\n",
    "  \n",
    "    if (Y_all.iloc[pindex].profit > CONST_PROFIT_THRESHOLD):\n",
    "        pstock_data = X_all.iloc[(pindex * dp.CONST_LOOKBACK_SAMPLES):((pindex +1) * dp.CONST_LOOKBACK_SAMPLES)]\n",
    "        title_postfix = ' ==  Profit: ' + str(round(Y_all.iloc[pindex].profit * 100, 1)) + '%,' + ' B/S: ' + Y_all.iloc[pindex].buy_date + ' / ' + Y_all.iloc[pindex].sell_date\n",
    "        plot_stock_data(pstock_data, title_postfix)\n",
    "        break\n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Y_all's buy_date and sell_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Y_all's buy_date and sell_date\n",
    "# convert date to num\n",
    "Y_all['mpl.buy_date'] = date_to_num(Y_all['buy_date'].values)\n",
    "Y_all['mpl.sell_date'] = date_to_num(Y_all['sell_date'].values)\n",
    "Y_all['hold_days'] = Y_all['mpl.sell_date'] - Y_all['mpl.buy_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check, iterrate over Y_all\n",
    "i = 0\n",
    "for index, row in Y_all.iterrows():\n",
    "    if row['hold_days'] == 0:\n",
    "        print(\"ERROR\")\n",
    "        print(index, row['hold_days'], row['profit'])\n",
    "        i += 1\n",
    "print('Total', i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate Y_all 'profit'\n",
    "Y_all['profit'] = Y_all['sell_price']/Y_all['buy_price'] - 1\n",
    "\n",
    "# Calculate Y_all 'profit.per.day'\n",
    "Y_all['profit.per.day'] = Y_all['profit']/Y_all['hold_days']\n",
    "\n",
    "Y_all.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep 'profit', which is a percentage rate of profit\n",
    "Y_all_cleanup = Y_all.drop(['mpl.buy_date', 'mpl.sell_date', 'hold_days', 'buy_date', 'buy_price', 'code', 'sell_date', 'sell_price', 'sell_reason'], axis=1)\n",
    "print(Y_all_cleanup.head(1))\n",
    "print(Y_all_cleanup.shape)\n",
    "print('No. of samples with positive profit higher than 20%: ', Y_all_cleanup[Y_all_cleanup.profit>0.2239].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Y_all 'hold_days' and 'profit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot 'hold_days' (Day) and 'profit' (%) relationship\n",
    "#  - colorbar indicates percentage (%) of earn per Day.\n",
    "\n",
    "# Y_profit = Y_all\n",
    "Y_profit = Y_all[Y_all['profit'] > 0.2239]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(Y_profit['hold_days'].values,  # hold days\n",
    "            Y_profit['profit.per.day'].values*100,  # profit percentage %\n",
    "            c=Y_profit['profit.per.day'],  # increment % per day\n",
    "            marker='o')\n",
    "plt.colorbar()\n",
    "plt.grid(True)\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Profit Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Y_all 'profit' Distribution Hisogram\n",
    "#### Percentile Y_all 'profit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Y_profit_np = Y_all_cleanup['profit'].values\n",
    "\n",
    "# quantile\n",
    "print(np.percentile(Y_profit_np,95))  # 95th percentile\n",
    "# percentile at 10% steps\n",
    "print(np.percentile(Y_profit_np, np.arange(0, 100, 10)))  # deciles\n",
    "\n",
    "print(Y_all_cleanup.shape[0], 'vs.', Y_all_cleanup[Y_all_cleanup.profit>=0.2239].shape[0])\n",
    "\n",
    "\n",
    "# sns.distplot(Y_all_cleanup['profit'].values, bins=50, kde=False, rug=False)\n",
    "sns.distplot(Y_all_cleanup[Y_all_cleanup.profit>0.2][Y_all_cleanup.profit<=0.3]['profit'].values, bins=10, kde=False, rug=False)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate sigmoid('profit' * 100 - 22.39 + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "t = Y_all['profit'].values  * 100 - 22.39 + 2\n",
    "\n",
    "print(t.shape)\n",
    "\n",
    "Y_all['sigmoid.profit'] = expit(Y_all['profit'].values  * 100 - 22.39 + 2)\n",
    "\n",
    "# print(Y_all['sigmoid.profit'])\n",
    "#sns.distplot(Y_all['sigmoid.profit'].values, bins=50, kde=False, rug=False)\n",
    "sns.distplot(t, bins=50, kde=False, rug=False)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "sns.distplot(Y_all_cleanup['profit'].values, bins=50, kde=False, rug=False)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate sigmoid('profit.per.day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "t = Y_all['profit.per.day'].values*100\n",
    "print(t.shape)\n",
    "\n",
    "print(np.isnan(t).any())\n",
    "\n",
    "Y_all['profit.per.day'].plot()\n",
    "plt.show()\n",
    "\n",
    "#sns.distplot(Y_all['profit.per.day'].values*100, bins=10, kde=False, rug=False)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#sns.distplot(Y_all_cleanup['profit'].values, bins=50, kde=False, rug=False)\n",
    "#plt.grid(True)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples Standadization\n",
    "\n",
    "归一化\n",
    "\n",
    "\n",
    "INPUT\n",
    "=====\n",
    "X_all: DataFrame with all samples and all features.\n",
    "\n",
    "OUTPUT\n",
    "======\n",
    "X_all_cleanup: DataFrame with all samples,\n",
    "    - but trimming off unnessary features. 'ATR', 'LOW10', etc. are dropped.\n",
    "\n",
    "X_all_cleanup_std: DataFrame,\n",
    "    - which keep one sample in every CONST_ONE_OUT_OF_NUMBER samples.\n",
    "    - with all features standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column 'ATR', 'LOW10', 'open', 'high', 'low', 'code'\n",
    "X_all_cleanup = X_all.drop(['ATR', 'LOW10', 'open', 'high', 'low', 'code'], axis=1)\n",
    "\n",
    "# print(X_all_cleanup.head())\n",
    "# X_all_cleanup.shape\n",
    "print(X_all_cleanup.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick up X samples, and Standardize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "absscaler = MaxAbsScaler()\n",
    "\n",
    "CONST_ONE_OUT_OF_NUMBER = 1\n",
    "\n",
    "# Note:\n",
    "#   - 'close', 'EMA8', 'EMA21' these three are correlated, and should be rescaled based on same value.\n",
    "CONST_LOOKBACK_SAMPLES = 60\n",
    "\n",
    "total_samples = int(X_all_cleanup.shape[0] / CONST_LOOKBACK_SAMPLES)\n",
    "X_all_cleanup_std = pd.DataFrame()  # initialize a dataframe.\n",
    "\n",
    "std_sample_list = []  # define a list for standadized samples. element is pandas DataFrame.\n",
    "\n",
    "# Initial call to print 0% progress\n",
    "dp.printProgressBar(0, total_samples - 1, prefix = 'Samples Standardization:', suffix = 'Complete', length = 60)\n",
    "\n",
    "for i in range(0, total_samples):\n",
    "    # only keep 1 out of CONST_ONE_OUT_OF_NUMBER\n",
    "#    if (i % CONST_ONE_OUT_OF_NUMBER != 0):\n",
    "#        continue\n",
    "#    if i >= 10000:  # tesing\n",
    "#        break\n",
    "    # slice out one sample\n",
    "    one_sample = X_all_cleanup.iloc[(i * CONST_LOOKBACK_SAMPLES):((i+1) * CONST_LOOKBACK_SAMPLES), :]\n",
    "    # if (i % 20 == 0):\n",
    "    #    print(one_sample.head(1))\n",
    "    # print(one_sample.shape)\n",
    "    # print(one_sample.columns)\n",
    "    # print(one_sample.head())\n",
    "    # print(one_sample.describe())\n",
    "    # copy to new sample skeleton\n",
    "    standardized_sample = pd.DataFrame(index=one_sample.index)\n",
    "    \n",
    "    # Scaling 'volume'\n",
    "    standardized_sample['volume'] = scaler.fit_transform(one_sample['volume'].values.reshape(-1,1))\n",
    "    \n",
    "    # Scaling 'close', 'EMA8', 'EMA21'. They are co-related, so scale from the same scale.\n",
    "    # print('close, EMA8, EMA21 scaling:')\n",
    "    pmin = one_sample['close'].min()\n",
    "    pmax = one_sample['close'].max()\n",
    "    standardized_sample['close'] = (one_sample['close'].values - pmin) / (pmax - pmin)\n",
    "    standardized_sample['EMA8'] = (one_sample['EMA8'].values - pmin) / (pmax - pmin)\n",
    "    standardized_sample['EMA21'] = (one_sample['EMA21'].values - pmin) / (pmax - pmin)\n",
    "    \n",
    "    # plot it\n",
    "    # fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(60, 15))\n",
    "    # ax1.set_title('Before Scaling')\n",
    "    # sns.kdeplot(one_sample['close'], ax=ax1)\n",
    "    # ax2.set_title('After Scaling')\n",
    "    # sns.kdeplot(standardized_sample['s_close'], ax=ax2)\n",
    "    # plt.show()\n",
    "    \n",
    "    # Scaling 'ADX'\n",
    "    standardized_sample['ADX'] = scaler.fit_transform(one_sample['ADX'].values.reshape(-1,1))\n",
    "    \n",
    "    # Copy 'SQUEEZE'\n",
    "    standardized_sample['SQUEEZE'] = one_sample['SQUEEZE']\n",
    "    \n",
    "    # Scaling 'MTMMA', 'HIST1' ~ 'HIST5', 'MACD6'\n",
    "    standardized_sample['MTMMA'] = absscaler.fit_transform(one_sample['MTMMA'].values.reshape(-1,1))\n",
    "    standardized_sample['HIST1'] = absscaler.fit_transform(one_sample['HIST1'].values.reshape(-1,1))\n",
    "    standardized_sample['HIST2'] = absscaler.fit_transform(one_sample['HIST2'].values.reshape(-1,1))\n",
    "    standardized_sample['HIST3'] = absscaler.fit_transform(one_sample['HIST3'].values.reshape(-1,1))\n",
    "    standardized_sample['HIST4'] = absscaler.fit_transform(one_sample['HIST4'].values.reshape(-1,1))\n",
    "    standardized_sample['HIST5'] = absscaler.fit_transform(one_sample['HIST5'].values.reshape(-1,1))\n",
    "    standardized_sample['MACD6'] = absscaler.fit_transform(one_sample['MACD6'].values.reshape(-1,1))\n",
    "    \n",
    "    # add it to std_sample_list\n",
    "    std_sample_list.append(standardized_sample)\n",
    "\n",
    "    # update progress bar\n",
    "    dp.printProgressBar(i, total_samples - 1, prefix = 'Samples Standardization:', suffix = 'Complete', length = 60)\n",
    "\n",
    "print('starting to concat ...')\n",
    "X_all_cleanup_std = pd.concat(std_sample_list)\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_all_cleanup_std.shape)\n",
    "print('Selected X samples: ', X_all_cleanup_std.shape[0]/CONST_LOOKBACK_SAMPLES)\n",
    "print(X_all_cleanup_std.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick up Y samples\n",
    "\n",
    "INPUT\n",
    "=====\n",
    "Y_all_cleanup: DataFrame with 'profit' as only column\n",
    "\n",
    "OUTPUT\n",
    "======\n",
    "Y_all_cleanup_std: DataFrame,\n",
    "    - based from Y_all_cleanup.\n",
    "    - which keep one sample in every CONST_ONE_OUT_OF_NUMBER samples.\n",
    "\n",
    "Y_all_binary_profit: DataFrame,\n",
    "    - based from Y_all_cleanup_std.\n",
    "    - change 'profit' to a binary value. int(1) or int(0).\n",
    "        - profit bigger than CONST_PROFIT_THRESHOLD is stored as 1.\n",
    "        - profit less than or equal to CONST_PROFIT_THRESHOLD is stored as 1.\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONST_PROFIT_THRESHOLD = 0.2239  # take any profit value less than this number as a LOSS.\n",
    "\n",
    "#\n",
    "# Y_all_cleanup_std\n",
    "#\n",
    "\n",
    "total_samples = int(X_all_cleanup.shape[0] / CONST_LOOKBACK_SAMPLES)\n",
    "total_samples_y = Y_all_cleanup.shape[0]\n",
    "\n",
    "if total_samples_y != total_samples:\n",
    "    print('ERROR: unmatched X and Y samples')\n",
    "\n",
    "Y_all_cleanup_std = pd.DataFrame()  # initialize a dataframe. Store profit rate as a float number.\n",
    "\n",
    "for i in range(0, total_samples_y):\n",
    "    # only keep 1 out of CONST_ONE_OUT_OF_NUMBER\n",
    "    if (i % CONST_ONE_OUT_OF_NUMBER != 0):\n",
    "        i +=1\n",
    "        continue;\n",
    "\n",
    "    # slice out one sample\n",
    "    one_sample = Y_all_cleanup.iloc[i:(i+1), :]\n",
    "    Y_all_cleanup_std = Y_all_cleanup_std.append(one_sample)\n",
    "\n",
    "#\n",
    "# Y_all_binary_profit\n",
    "#\n",
    "\n",
    "# create binary profit value: 0 for loss, 1 for win.\n",
    "Y_all_binary_profit = Y_all_cleanup_std.copy()\n",
    "\n",
    "for index, row in Y_all_cleanup_std.iterrows():\n",
    "        if row['profit'] > CONST_PROFIT_THRESHOLD:\n",
    "            Y_all_binary_profit.loc[index]['profit'] = int(1)\n",
    "        else:\n",
    "            Y_all_binary_profit.loc[index]['profit'] = int(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Y_all_cleanup_std: ', Y_all_cleanup_std.shape)\n",
    "print('Y_all_binary_profit: ', Y_all_binary_profit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_all_binary_profit[Y_all_binary_profit['profit']>0.5].count()  # 1 means Good Profit, 0 means Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Y_all_binary_profit to Categorical array\n",
    "\n",
    "INPUT:\n",
    "=====\n",
    "Y_all_binary_profit: Dataframe with binary 'profit' field.\n",
    "\n",
    "OUTPUT:\n",
    "=======\n",
    "Y_all_binary_categorical: Numpy data array in shape [x, 2]\n",
    "    - two category column. One for Good Profit, the other for Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "Y_all_binary_categorical = np_utils.to_categorical(Y_all_binary_profit)\n",
    "print(Y_all_binary_categorical)\n",
    "print(Y_all_binary_categorical.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Balanced samples for 'Good Profit' and 'Loss'\n",
    "\n",
    "There are 6626 samples of 'Good Profit', profit bigger than 22.39%. To balance that, choose 6626 (1 out of 9) samples of 'Loss' randomly from the X/Y datasets.\n",
    "\n",
    "Together, they make a balanced train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# pick_one_sample_x_and_y\n",
    "#\n",
    "\n",
    "\n",
    "def pick_one_sample_x_and_y(index, X_samples_all, Y_samples_all, len_X_sample=60, len_Y_sample=1):\n",
    "    ''' Given an index, return the sample's X and Y slice.\n",
    "    \n",
    "    Input\n",
    "    =====\n",
    "    index: a sequencial int number, denotes index no. of a sample\n",
    "    X_samples_all: DataFrame, X_all set, with \n",
    "    len_X_sample:  integer, is one X_sample's length. Default 60.\n",
    "    Y_samples_all: DataFrame  Y_all set, with \n",
    "    len_Y_sample:  integer, is one Y_sample's length. Default 1.\n",
    "        \n",
    "    Output\n",
    "    ======\n",
    "    one_sample_x: DataFrame\n",
    "        the sample's X part\n",
    "    one_sample_y: DataFrame\n",
    "        the sample's Y part\n",
    "    '''\n",
    "    one_sample_y = Y_samples_all.iloc[(index * len_Y_sample):((index+1) * len_Y_sample), :]\n",
    "    one_sample_x = X_samples_all.iloc[(index * len_X_sample):((index+1) * len_X_sample), :]\n",
    "\n",
    "    return one_sample_x, one_sample_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# generate balanced dataset\n",
    "#\n",
    "\n",
    "import random\n",
    "random.seed('2017-12-02')\n",
    "\n",
    "# test how many 'profit'/'loss' samples are selected\n",
    "i = 0  # 'profit'\n",
    "j = 0  # 'loss'\n",
    "\n",
    "# init\n",
    "X_all_picked = pd.DataFrame()  # initialize a dataframe.\n",
    "Y_all_picked = pd.DataFrame()  # initialize a dataframe.\n",
    "\n",
    "X_frame = []  # collection of X sample dataframes.\n",
    "Y_frame = []  # collection of Y sample dataframes.\n",
    "\n",
    "for index, row in Y_all_binary_profit.iterrows():\n",
    "    # generate a random number between 1 and 9, inclusive\n",
    "    selector = random.randint(1, 9)\n",
    "    \n",
    "    if (row['profit'] == 0) and (selector <= 8):\n",
    "        # skip\n",
    "        continue\n",
    "    \n",
    "    # pick it\n",
    "    one_x_sample, one_y_sample = pick_one_sample_x_and_y(index,\n",
    "                                                         X_all_cleanup_std,  # X samples all\n",
    "                                                         Y_all_binary_profit,  # Y samples all\n",
    "                                                         dp.CONST_LOOKBACK_SAMPLES,  # len of each X sample\n",
    "                                                         1  # len of each Y sample\n",
    "                                                        )\n",
    "    \n",
    "    # debug code\n",
    "    if (row['profit'] == 1):\n",
    "        i += 1\n",
    "    else:\n",
    "        j += 1\n",
    "        \n",
    "    # add it to X_frame and Y_frame\n",
    "    X_frame.append(one_x_sample)\n",
    "    Y_frame.append(one_y_sample)\n",
    "\n",
    "X_all_picked = pd.concat(X_frame)\n",
    "Y_all_picked = pd.concat(Y_frame)\n",
    "\n",
    "'''\n",
    "iterrate Y_all_cleanup_std by index,\n",
    "    if profit > 22.39%,\n",
    "        pick it;\n",
    "    else,\n",
    "        generate a random int between 0 and 8, round((total sample numbers -profit sample numbers)/(profit sample numbers))\n",
    "        if randomint = 0\n",
    "            pick it;\n",
    "        else,\n",
    "            skip it;\n",
    "    endif\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i,j, i+j)\n",
    "print('X picked: ', X_all_picked.shape[0]/dp.CONST_LOOKBACK_SAMPLES)\n",
    "print('Y picked: ', Y_all_picked.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split to Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#\n",
    "# splict_train_and_test\n",
    "#\n",
    "\n",
    "\n",
    "def splict_train_and_test(X_samples_all, Y_samples_all, len_X_sample=60, len_Y_sample=1, train_test_ratio=7):\n",
    "    ''' Split samples into X_train, Y_train and X_test, Y_test.\n",
    "    \n",
    "    Input\n",
    "    =====\n",
    "    X_samples_all: DataFrame, X_all set \n",
    "    len_X_sample:  integer, is one X_sample's length. Default 60.\n",
    "    Y_samples_all: DataFrame  Y_all set \n",
    "    len_Y_sample:  integer, is one Y_sample's length. Default 1.\n",
    "        \n",
    "    train_test_ratio: integer, \n",
    "        The ration is defined in train_test_ratio, it's an integer between 1 and 10, inclusive.\n",
    "        It means in every 10 samples, how many to be used as train.\n",
    "        And others will be as test.\n",
    "        \n",
    "    Output\n",
    "    ======\n",
    "    X_train, Y_train: DataFrame\n",
    "    X_test,  Y_test : DataFrame\n",
    "    '''\n",
    "\n",
    "    n_total_samples = int(X_samples_all.shape[0] / len_X_sample)\n",
    "\n",
    "    Xtn_frame = []  # X_train\n",
    "    Ytn_frame = []  # Y_train\n",
    "    Xtt_frame = []  # X_test\n",
    "    Ytt_frame = []  # Y_test\n",
    "    \n",
    "    for index in range(0, n_total_samples):\n",
    "        # slice out one sample\n",
    "        one_x_sample, one_y_sample = pick_one_sample_x_and_y(index,\n",
    "                                                             X_samples_all,  # X samples all\n",
    "                                                             Y_samples_all,  # Y samples all\n",
    "                                                             len_X_sample,  # len of each X sample\n",
    "                                                             len_Y_sample  # len of each Y sample\n",
    "                                                            )\n",
    "\n",
    "        # generate a random number\n",
    "        selector = random.randint(1, 10)\n",
    "\n",
    "        if selector <= train_test_ratio:\n",
    "            # put this sample to X_train, and Y_train\n",
    "            Xtn_frame.append(one_x_sample)\n",
    "            Ytn_frame.append(one_y_sample)\n",
    "        else:\n",
    "            # put this sample to X_test, and Y_test\n",
    "            Xtt_frame.append(one_x_sample)\n",
    "            Ytt_frame.append(one_y_sample)\n",
    "\n",
    "    print('start concat...')\n",
    "    # concat\n",
    "    X_train = pd.concat(Xtn_frame)\n",
    "    Y_train = pd.concat(Ytn_frame)\n",
    "    X_test = pd.concat(Xtt_frame)\n",
    "    Y_test = pd.concat(Ytt_frame)\n",
    "    \n",
    "    print('Done!')\n",
    "    return X_train, Y_train, X_test, Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = splict_train_and_test(X_all_picked, Y_all_picked, dp.CONST_LOOKBACK_SAMPLES, 1, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of X_train/Y_train: \", int(X_train.shape[0] / dp.CONST_LOOKBACK_SAMPLES), Y_train.shape[0])\n",
    "print(\"Size of X_test/Y_test: \", int(X_test.shape[0] / dp.CONST_LOOKBACK_SAMPLES), Y_test.shape[0])\n",
    "\n",
    "print('How many have positive profit in Y_train?: ', Y_train[Y_train.profit>0].count())\n",
    "print('How many have positive profit in Y_test?: ', Y_test[Y_test.profit>0].count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save standardized X/Y samples to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save X sample \n",
    "# X_all_cleanup_std.to_csv('hs300_20100101-20171124-samples-cleanup-std.csv')\n",
    "#print(all_data.shape)\n",
    "#print(all_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert X/Y Train/Test to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "X_train_np = X_train.values.reshape(-1, 60, 13)\n",
    "X_test_np = X_test.values.reshape(-1, 60, 13)\n",
    "\n",
    "Y_train_np = Y_train.values\n",
    "Y_test_np = Y_test.values\n",
    "\n",
    "\n",
    "# Y_train_np = np_utils.to_categorical(Y_train)\n",
    "# Y_test_np = np_utils.to_categorical(Y_test)\n",
    "\n",
    "print('Train X/Y: ', X_train_np.shape, Y_train_np.shape)\n",
    "print('Test  X/Y: ', X_test_np.shape, Y_test_np.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建 Keras 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD PACKAGES \n",
    "from numpy.random import seed\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv1D 模型\n",
    "\n",
    "##### Common Settings:\n",
    "- Number of filters: (power of 2, e.g. 32, 64, 128, 512)\n",
    "- Kernel size: 3, 5, 1\n",
    "- Stride: 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# apply a convolution 1d of length 3 to a sequence with CONST_LOOKBACK_SAMPLES timesteps, each with 13-dimensions,\n",
    "# with 256 output filters\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=256, kernel_size=3, padding='valid', input_shape=(dp.CONST_LOOKBACK_SAMPLES, 13)))  # , activation='tanh'\n",
    "print('Model input shape: ', model.input_shape)\n",
    "print('Conv1D, 512', model.output_shape)\n",
    "\n",
    "#model.add(Conv1D(filters=256, kernel_size=3, padding='same'))  # , activation='tanh'\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=3, padding='same'))  # , activation='tanh'\n",
    "print('Conv1D, 128', model.output_shape)\n",
    "\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same'))  # , activation='tanh'\n",
    "print('Conv1D, 32: ', model.output_shape)\n",
    "\n",
    "# we use max pooling:\n",
    "model.add(MaxPooling1D(2))\n",
    "print('MaxPooling1D by 2: model output shape: ', model.output_shape)\n",
    "\n",
    "model.add(Flatten())\n",
    "print('Flatten： ', model.output_shape)\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(896, kernel_initializer='normal'))  # , activation='tanh'))\n",
    "print('Dense, hidden_dims: model output shape: ', model.output_shape)\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(16, kernel_initializer='normal'))  # , activation='tanh'))\n",
    "print('Dense, hidden_dims 16: model output shape: ', model.output_shape)\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "print('Dropout 0.2: model output shape: ', model.output_shape)\n",
    "\n",
    "# We project onto a single unit output layer:\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print('Dense 1: ', model.output_shape)\n",
    "\n",
    "# sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#              optimizer='adam',\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "# model.add(Activation('sigmoid'))\n",
    "# print('Activation sigmoid: ', model.output_shape)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Training\n",
    "#### Continue previous Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training, or\n",
    "# Continue prvious training: don't do model.compile() which will reset the inner state of the optimizer \n",
    "#\n",
    "\n",
    "# K.clear_session()\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "model.fit(X_train_np, Y_train_np,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test_np, Y_test_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend import tensorflow_backend as K\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(\n",
    "                    intra_op_parallelism_threads=0)) as sess:\n",
    "    K.set_session(sess)   # your keras code follows\n",
    "\n",
    "    # apply a convolution 1d of length 3 to a sequence with CONST_LOOKBACK_SAMPLES timesteps, each with 13-dimensions,\n",
    "    # with 256 output filters\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=256, kernel_size=3, padding='valid', input_shape=(dp.CONST_LOOKBACK_SAMPLES, 13)))  # , activation='tanh'\n",
    "    print('Model input shape: ', model.input_shape)\n",
    "    print('Conv1D, 512', model.output_shape)\n",
    "\n",
    "    #model.add(Conv1D(filters=256, kernel_size=3, padding='same'))  # , activation='tanh'\n",
    "\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, padding='same'))  # , activation='tanh'\n",
    "    print('Conv1D, 128', model.output_shape)\n",
    "\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same'))  # , activation='tanh'\n",
    "    print('Conv1D, 32: ', model.output_shape)\n",
    "\n",
    "    # we use max pooling:\n",
    "    model.add(MaxPooling1D(2))\n",
    "    print('MaxPooling1D by 2: model output shape: ', model.output_shape)\n",
    "\n",
    "    model.add(Flatten())\n",
    "    print('Flatten： ', model.output_shape)\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(896, kernel_initializer='normal'))  # , activation='tanh'))\n",
    "    print('Dense, hidden_dims: model output shape: ', model.output_shape)\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(16, kernel_initializer='normal'))  # , activation='tanh'))\n",
    "    print('Dense, hidden_dims 16: model output shape: ', model.output_shape)\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "    print('Dropout 0.2: model output shape: ', model.output_shape)\n",
    "\n",
    "    # We project onto a single unit output layer:\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print('Dense 1: ', model.output_shape)\n",
    "\n",
    "    sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    batch_size = 32\n",
    "    epochs = 1\n",
    "    model.fit(X_train_np, Y_train_np,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test_np, Y_test_np))\n",
    "    \n",
    "    # clear the session, to avoid session confusion.\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model and Weights to disk\n",
    "\n",
    "Ref: https://machinelearningmastery.com/save-load-keras-deep-learning-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# file names\n",
    "model_jason_file_name = 'model-2017-12-03-v2.json'\n",
    "model_weights_hdf5 = 'model-2017-12-03-v2.h5'\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(model_jason_file_name, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(model_weights_hdf5)\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# load json and create model\n",
    "json_file = open(model_jason_file_name, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(model_weights_hdf5)\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use loaded model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate loaded model on test data\n",
    "#loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "#score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "#print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use NN to Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: fetch todays' K Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_list_hs300 = ts.get_hs300s()  # 沪深300成分股\n",
    "\n",
    "sd_t = '2015-05-01'  # start date\n",
    "ed_t = '2017-12-02'  # end date\n",
    "\n",
    "# fetch all data in the list. _t means 'today'\n",
    "all_data_t = dp.fetch_raw_data(stock_list_hs300, sd_t, ed_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stock_list_hs300.head(2))\n",
    "print(all_data_t.tail(2))\n",
    "print(all_data_t.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: all_data, MultiIndex'ed by 'code' and 'date'\n",
    "#\n",
    "\n",
    "CONST_DROP_THRESHOLD_t = 440  # Wave C needs 377, sample lookback needs 60.\n",
    "\n",
    "all_data_and_features_t = pd.DataFrame()  # stock data with all available features, such as 'SQZ', 'HIST1', and all.\n",
    "\n",
    "\n",
    "stock_list = all_data_t.groupby(level=0).size().reset_index(name='counts')\n",
    "# Initial call to print 0% progress\n",
    "total = stock_list.shape[0]\n",
    "i = 0\n",
    "dp.printProgressBar(i, total, prefix = 'Progress:', suffix = 'Complete', length = 60)\n",
    "\n",
    "\n",
    "for index, row in stock_list.iterrows():  # iterate the stock list\n",
    "    # slicing one stock_data\n",
    "    stock_data = all_data_t.loc[row['code']]\n",
    "    \n",
    "    # skip if stock_data has less than CONST_DROP_THRESHOLD bars\n",
    "    if stock_data.shape[0] < CONST_DROP_THRESHOLD_t:\n",
    "        i += 1\n",
    "        print('DROP ' + row['code'])\n",
    "        continue\n",
    "        \n",
    "    # print(row['code'], row['name'])\n",
    "    # print(stock_data.shape)\n",
    "    \n",
    "    # add 'EMA8', 'EMA21'\n",
    "    stock_data = stock_data.join(sb.ttm_propulsion(stock_data))\n",
    "    # add SQZ\n",
    "    stock_data = stock_data.join(sb.ttm_squeeze(stock_data))\n",
    "    # add WAVE\n",
    "    stock_data = stock_data.join(sb.ttm_wave(stock_data))\n",
    "    # add ADX\n",
    "    stock_data = stock_data.join(sb.talib_adx(stock_data))\n",
    "    # add ATR\n",
    "    stock_data = stock_data.join(sb.talib_atr(stock_data))\n",
    "    # add N-bar LOW\n",
    "    stock_data = stock_data.join(sb.talib_nbarlow(stock_data, N_BAR_LOWEST = 10))\n",
    "    \n",
    "    # append to all_data_and_features\n",
    "    all_data_and_features_t = all_data_and_features_t.append(stock_data)\n",
    "    \n",
    "    # update progress bar\n",
    "    i += 1\n",
    "    dp.printProgressBar(i, total, prefix = 'Progress:', suffix = 'Complete', length = 60)\n",
    "\n",
    "# MulitIndex with both 'code' and 'date'\n",
    "all_data_and_features_t.set_index([all_data_and_features_t['code'], all_data_and_features_t.index], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('original all_data: ')\n",
    "print(all_data_t.columns, '\\n', all_data_t.shape)\n",
    "\n",
    "print('after adding featuresa: ')\n",
    "print(all_data_and_features_t.columns, '\\n', all_data_and_features_t.shape)\n",
    "print(all_data_and_features_t.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: In today's K Line, find valid buy samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is reused from dp.generate_samples()\n",
    "\n",
    "#\n",
    "# generate_buy_samples_given_index\n",
    "#\n",
    "\n",
    "# number of bars to look back to form a sample\n",
    "# CONST_LOOKBACK_SAMPLES = 60\n",
    "\n",
    "def generate_buy_samples_given_index(stock_data, index):\n",
    "    ''' Function to generate buy samples for the given 'index'. Index is a datetime value. \n",
    "        选出在指定时点下，符合买入规则的数据用例.\n",
    "   \n",
    "    Explain\n",
    "    ======= \n",
    "    Given a stock_data, generate buy samples on the given datetime 'index'.\n",
    "        Currently, using is_squeeze_buy_point() to find buy points.\n",
    "        \n",
    "    Input\n",
    "    =====\n",
    "    stock_data: DataFrame\n",
    "        stock_data with full features 'SQZ', WAVE C/B/A, ATR, ADX, etc.\n",
    "    index: string\n",
    "\n",
    "    Output\n",
    "    ======\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Initialize X_all\n",
    "    X_all = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    \n",
    "    if not sb.is_squeeze_buy_point(stock_data, index):\n",
    "        ## print(index + ' is NOT')\n",
    "        return X_all\n",
    "    \n",
    "    # Yes, this is a squeeze buy point\n",
    "    # Back fetch N-record\n",
    "    #\n",
    "    location_of_buy_point = stock_data.index.get_loc(index)\n",
    "    first_location = location_of_buy_point - CONST_LOOKBACK_SAMPLES + 1\n",
    "    if first_location < 0: # there is no enough records to form a valid sample\n",
    "        # skip it\n",
    "        return X_all\n",
    "    \n",
    "    # Slicing. These are totally CONST_LOOKBACK_SAMPLES of records.\n",
    "    x_sample = stock_data.iloc[first_location:(location_of_buy_point + 1)]\n",
    "    # x_sample Validity check.\n",
    "    if x_sample.isnull().values.any():\n",
    "        # skip it\n",
    "        return X_all\n",
    "\n",
    "    # Add N-record to X_all\n",
    "    X_all = X_all.append(x_sample, ignore_index = False)\n",
    "\n",
    "    return X_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_X_all_t(today_index):\n",
    "\n",
    "    # Initialize X_all\n",
    "    X_all_t = pd.DataFrame()\n",
    "\n",
    "    print('checking: ', today_index)\n",
    "    # Initial call to print 0% progress\n",
    "    total = all_data_and_features_t.groupby(level=0).size().shape[0]\n",
    "    i = 0\n",
    "    dp.printProgressBar(i, total, prefix = 'Progress:', suffix = 'Complete', length = 60)\n",
    "\n",
    "    for code, onestock_df in all_data_and_features_t.groupby(level=0):\n",
    "        # print(code, ' ', onestock_df.shape)\n",
    "\n",
    "        # generate a MultiIndex tuple    \n",
    "        this_index = (code, today_index)\n",
    "\n",
    "        # check to know this index doesn't exist?\n",
    "        if this_index not in onestock_df.index:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        onestock_X_all = generate_buy_samples_given_index(onestock_df, this_index)\n",
    "        # print(onestock_X_all.shape)\n",
    "\n",
    "        # Add samples to X_all\n",
    "        X_all_t = X_all_t.append(onestock_X_all, ignore_index = False)\n",
    "\n",
    "        # update progress\n",
    "        i += 1\n",
    "        dp.printProgressBar(i, total, prefix = 'Progress:', suffix = 'Complete', length = 60)\n",
    "\n",
    "        # testing\n",
    "        # if i > 50:\n",
    "        #    break\n",
    "    return X_all_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize X_all\n",
    "X_all_t = pd.DataFrame()\n",
    "\n",
    "'''\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-01'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-02'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-03'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-06'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-07'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-08'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-09'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-10'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-13'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-14'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-15'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-16'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-17'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-20'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-21'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-22'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-23'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-24'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-27'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-28'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-29'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-11-30'), ignore_index=False)\n",
    "'''\n",
    "\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-12-01'), ignore_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize X_all\n",
    "X_all_t = pd.DataFrame()\n",
    "\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-10-09'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-10-10'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-10-11'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-10-12'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-10-13'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-10-16'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-10-17'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-10-18'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-10-19'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-10-20'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-10-23'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-10-24'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-10-25'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-10-26'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-10-27'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-10-30'), ignore_index=False)\n",
    "X_all_t = X_all_t.append(get_X_all_t('2017-10-31'), ignore_index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of valid samples: ', X_all_t.shape[0]/dp.CONST_LOOKBACK_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Standardization X_all_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPY'ed from above\n",
    "\n",
    "# drop column 'ATR', 'LOW10', 'open', 'high', 'low', 'code'\n",
    "X_all_cleanup_t = X_all_t.drop(['ATR', 'LOW10', 'open', 'high', 'low', 'code'], axis=1)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "absscaler = MaxAbsScaler()\n",
    "\n",
    "# Note:\n",
    "#   - 'close', 'EMA8', 'EMA21' these three are correlated, and should be rescaled based on same value.\n",
    "#CONST_LOOKBACK_SAMPLES = 60\n",
    "\n",
    "total_samples = int(X_all_cleanup_t.shape[0] / CONST_LOOKBACK_SAMPLES)\n",
    "X_all_cleanup_std_t = pd.DataFrame()  # initialize a dataframe.\n",
    "\n",
    "# Initial call to print 0% progress\n",
    "dp.printProgressBar(0, total_samples - 1, prefix = 'Samples Standardization:', suffix = 'Complete', length = 60)\n",
    "\n",
    "for i in range(0, total_samples):\n",
    "    # slice out one sample\n",
    "    one_sample = X_all_cleanup_t.iloc[(i * CONST_LOOKBACK_SAMPLES):((i+1) * CONST_LOOKBACK_SAMPLES), :]\n",
    "    # if (i % 20 == 0):\n",
    "    #    print(one_sample.head(1))\n",
    "    # print(one_sample.shape)\n",
    "    # print(one_sample.columns)\n",
    "    # print(one_sample.head())\n",
    "    # print(one_sample.describe())\n",
    "    # copy to new sample skeleton\n",
    "    standardized_sample = pd.DataFrame(index=one_sample.index)\n",
    "    \n",
    "    # Scaling 'volume'\n",
    "    standardized_sample['volume'] = scaler.fit_transform(one_sample['volume'].values.reshape(-1,1))\n",
    "    \n",
    "    # Scaling 'close', 'EMA8', 'EMA21'. They are co-related, so scale from the same scale.\n",
    "    # print('close, EMA8, EMA21 scaling:')\n",
    "    pmin = one_sample['close'].min()\n",
    "    pmax = one_sample['close'].max()\n",
    "    standardized_sample['close'] = (one_sample['close'].values - pmin) / (pmax - pmin)\n",
    "    standardized_sample['EMA8'] = (one_sample['EMA8'].values - pmin) / (pmax - pmin)\n",
    "    standardized_sample['EMA21'] = (one_sample['EMA21'].values - pmin) / (pmax - pmin)\n",
    "    \n",
    "    # plot it\n",
    "    # fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(60, 15))\n",
    "    # ax1.set_title('Before Scaling')\n",
    "    # sns.kdeplot(one_sample['close'], ax=ax1)\n",
    "    # ax2.set_title('After Scaling')\n",
    "    # sns.kdeplot(standardized_sample['s_close'], ax=ax2)\n",
    "    # plt.show()\n",
    "    \n",
    "    # Scaling 'ADX'\n",
    "    standardized_sample['ADX'] = scaler.fit_transform(one_sample['ADX'].values.reshape(-1,1))\n",
    "    \n",
    "    # Copy 'SQUEEZE'\n",
    "    standardized_sample['SQUEEZE'] = one_sample['SQUEEZE']\n",
    "    \n",
    "    # Scaling 'MTMMA', 'HIST1' ~ 'HIST5', 'MACD6'\n",
    "    standardized_sample['MTMMA'] = absscaler.fit_transform(one_sample['MTMMA'].values.reshape(-1,1))\n",
    "    standardized_sample['HIST1'] = absscaler.fit_transform(one_sample['HIST1'].values.reshape(-1,1))\n",
    "    standardized_sample['HIST2'] = absscaler.fit_transform(one_sample['HIST2'].values.reshape(-1,1))\n",
    "    standardized_sample['HIST3'] = absscaler.fit_transform(one_sample['HIST3'].values.reshape(-1,1))\n",
    "    standardized_sample['HIST4'] = absscaler.fit_transform(one_sample['HIST4'].values.reshape(-1,1))\n",
    "    standardized_sample['HIST5'] = absscaler.fit_transform(one_sample['HIST5'].values.reshape(-1,1))\n",
    "    standardized_sample['MACD6'] = absscaler.fit_transform(one_sample['MACD6'].values.reshape(-1,1))\n",
    "    \n",
    "    X_all_cleanup_std_t = X_all_cleanup_std_t.append(standardized_sample)\n",
    "    dp.printProgressBar(i, total_samples - 1, prefix = 'Samples Standardization:', suffix = 'Complete', length = 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_all_cleanup_std_t.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_all_cleanup_std_t.values.reshape(-1, 60, 13).shape)\n",
    "result = model.predict(X_all_cleanup_std_t.values.reshape(-1, 60, 13), batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make Y_all_std_t \n",
    "yindex=range(0, int(X_all_cleanup_std_t.shape[0]/dp.CONST_LOOKBACK_SAMPLES))\n",
    "\n",
    "Y_all_std_predicted_t = pd.DataFrame(index=yindex, columns=['predict.profit'])\n",
    "\n",
    "Y_all_std_predicted_t['predict.profit'] = result\n",
    "print(Y_all_std_predicted_t.sort_values(['predict.profit']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Visaulize it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after 60 epochs\n",
    "# 9347/9347 [==============================] - 65s - loss: 0.4525 - acc: 0.7912 - val_loss: 0.8121 - val_acc: 0.6486\n",
    "\n",
    "# pindex = 2  # 2 潍柴动力 # 27 中信银行 # 3 阳光城 # 5 分众传媒 # 22 工商银行 # 5  # 3  # 27  # 2  # Profit Good (2 is max)\n",
    "# pindex = # 15 伊利股份 # 7 立讯精密 # 4 双汇发展 #21 广深铁路  # 16 长江电力 # Loss (16 is less)\n",
    "\n",
    "# after another 100 epochs, accu=0.\n",
    "# 9347/9347 [==============================] - 59s - loss: 0.3649 - acc: 0.8394 - val_loss: 1.0057 - val_acc: 0.6653.\n",
    "\n",
    "pindex = 18  # 0 华侨城 # 5 分众传媒 # 10 上港集团 # 1 美的集团 # 22 工商银行 # 3 阳光城 # 18 大秦铁路 # Profit Good (18 is max)\n",
    "# pindex = 7  # 7 立讯精密 # 9 包钢股份  # 11 中国石化 # 16 长江电力 # 8 上海机场 # 21 广深铁路 # 12 万华化学 # Loss (12 is less)\n",
    "\n",
    "# make a title\n",
    "title_postfix = ' == Predicted (Profit +22%) Confidence Level: ' + str(Y_all_std_predicted_t.iloc[pindex]['predict.profit'])\n",
    "\n",
    "# plot it\n",
    "plot_one_x_sample(pindex, X_all_t, dp.CONST_LOOKBACK_SAMPLES, title_postfix)\n",
    "\n",
    "# plot_one_x_sample(pindex, X_all_cleanup_std_t, dp.CONST_LOOKBACK_SAMPLES, title_postfix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# plot_one_x_sample\n",
    "#\n",
    "\n",
    "\n",
    "def plot_one_x_sample(pindex, X_samples_all, len_X_sample=60, title_postfix=''):\n",
    "    ''' Plot one X sample.\n",
    "    \n",
    "    Input\n",
    "    =====\n",
    "    pindex: Integer. the index X sample will be plot.\n",
    "    X_samples_all: DataFrame, X_all set \n",
    "    len_X_sample:  integer, is one X_sample's length. Default 60.\n",
    "\n",
    "    title_postfix: String, a title postfix\n",
    "    \n",
    "    Output\n",
    "    ======\n",
    "    None\n",
    "    '''\n",
    "    # validity check\n",
    "    if (pindex >=  X_samples_all.shape[0] / len_X_sample) or (index <0):\n",
    "        print('Error: Wrong Input index')\n",
    "        return\n",
    "\n",
    "    pstock_data = X_samples_all.iloc[(pindex * len_X_sample):((pindex +1) * len_X_sample)]\n",
    "    plot_stock_data(pstock_data, title_postfix)\n",
    "    \n",
    "    return\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# stock_code_to_name\n",
    "#\n",
    "\n",
    "def stock_code_to_name(code, stock_list):\n",
    "    ''' From stock 'code' to get stock 'name'\n",
    "    \n",
    "    Input\n",
    "    =====\n",
    "    code: stock code, eg. '000338'\n",
    "    stock_list: the stock list to search for\n",
    "    \n",
    "    Output\n",
    "    ======\n",
    "    name: string, of the stock name. eg. '潍柴动力'\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解决 matplotlib 中文显示的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['font.sans-serif'] = ['SimHei']  # 指定默认字体  \n",
    "mpl.rcParams['axes.unicode_minus'] = False  # 解决保存图像是负号'-'显示为方块的问题  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = all_data.loc['002839']\n",
    "print(df1.shape)\n",
    "\n",
    "# add 1st featue\n",
    "df1_propulsion = ttm_propulsion(df1)\n",
    "# a = df1_propulsion.join(df1.drop(['code'], axis=1))\n",
    "a = df1.join(df1_propulsion)\n",
    "\n",
    "# a.rename(columns={'EMA8':'EMA8a', 'EMA21':'EMA21a'}, inplace=True)\n",
    "\n",
    "# add 2nd feature，concatination\n",
    "b = a.join(ttm_squeeze(a))\n",
    "\n",
    "# add 3rd feature, concat. TTM Wave C/B/A\n",
    "d = b.join(ttm_wave(b))\n",
    "\n",
    "print(d.columns)\n",
    "print(d.describe())\n",
    "\n",
    "sd = '2017-05-04'\n",
    "ed = '2017-12-04'\n",
    "print(\"TTM Wave C:\")\n",
    "d.loc[sd:ed][['HIST5', 'MACD6']].plot()\n",
    "plt.show()\n",
    "#print(d.loc['2017-09-10':ed][['close', 'HIST5', 'MACD6', 'SQUEEZE', 'MTMMA']])\n",
    "\n",
    "print(\"TTM Wave B:\")\n",
    "d.loc[sd:ed][['HIST3', 'HIST4']].plot()\n",
    "plt.show()\n",
    "#print(d.loc[sd:ed][['close', 'EMA8', 'EMA21', 'HIST3', 'HIST4']])\n",
    "\n",
    "print(\"TTM Wave A:\")\n",
    "d.loc[sd:ed][['HIST1', 'HIST2']].plot()\n",
    "plt.show()\n",
    "#print(d.loc[sd:ed][['close', 'EMA8', 'EMA21', 'HIST1', 'HIST2']])\n",
    "\n",
    "# add 4th feature, concat. ADX\n",
    "e = d.join(talib_adx(d))\n",
    "print(e.columns)\n",
    "print(\"ADX:\")\n",
    "e.loc[sd:ed]['ADX'].plot()\n",
    "plt.show()\n",
    "e.loc[sd:ed][['close']].plot()\n",
    "plt.show()\n",
    "print(e.loc[sd:ed][['ADX', 'SQUEEZE', 'MTMMA']])\n",
    "\n",
    "# add 5th feature, N-Bar low\n",
    "f = e.join(talib_nbarlow(e, 10))\n",
    "print(f.columns)\n",
    "f.loc[sd:ed][['close', 'LOW10']].plot()\n",
    "plt.show()\n",
    "\n",
    "# add 6th feaure, ATR\n",
    "\n",
    "g = f.join(talib_atr(f))\n",
    "print(g.columns)\n",
    "g.loc[sd:ed][['ATR']].plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_list_sz50 = ts.get_sz50s()\n",
    "stock_list_sz50.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取日K线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = ts.get_k_data('000001')\n",
    "df2 = ts.get_k_data('000002')\n",
    "df1.head()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.set_index('date', inplace=True)\n",
    "print(df.head(2))\n",
    "df['close'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Panel 学习\n",
    "\n",
    "- Ref: Panel 结构： http://www.jianshu.com/p/424e61c2f8b8\n",
    "- Ref: 米筐 Pandas 教程： https://www.ricequant.com/community/topic/3558/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = {'000001' : pd.DataFrame(np.random.randn(10, 3)),'000002' : pd.DataFrame(np.random.randn(3, 5))}\n",
    "pnl = pd.Panel(data)\n",
    "print(pnl['000001'].describe())\n",
    "print(pnl['000002'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine df1 and df2 into Panel\n",
    "data = {'000001' : df1,'000002' : df2}\n",
    "pnl = pd.Panel(data)\n",
    "# panel['']\n",
    "print(pnl)\n",
    "\n",
    "dfextracted = pnl['000001']\n",
    "dfextracted.set_index('date', inplace=True)\n",
    "dfextracted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取近期5分钟行情\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m5df = ts.get_k_data('600000', ktype='5')\n",
    "m5df.head()\n",
    "\n",
    "newdf = m5df[[]]\n",
    "newdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join， 选取指定日期的数据\n",
    "\n",
    "给定df，包含全部日期数据。\n",
    "给定日期范围dates。\n",
    "选出dates内的数据：df_dates。\n",
    "\n",
    "只使用‘close’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "start_date = '2017-01-01'\n",
    "end_date = '2017-01-31'\n",
    "dates = pd.date_range(start_date, end_date)\n",
    "df_dates = pd.DataFrame(index=dates)\n",
    "df_dates.shape\n",
    "# print(df_dates)\n",
    "\n",
    "# slicing, use only 'close', and rename it to 'stock ID'\n",
    "df_dates = df_dates.join(df['close'])\n",
    "df_dates.rename(columns={'close':'600000'}, inplace=True)\n",
    "df_dates.dropna(inplace=True)\n",
    "print(df_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习 MultiIndex Dataframe\n",
    "\n",
    "- Ref： http://blog.csdn.net/tpoy0099/article/details/49074551"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: fetch_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tushare as ts\n",
    "#Eg.\n",
    "# stock_list_hs300 = ts.get_hs300s()\n",
    "stock_list_test = pd.DataFrame([{'code':'000001'}, {'code':'000002'}])\n",
    "print(stock_list_test)\n",
    "\n",
    "dp.printProgressBar(0, total, prefix = 'Progress:', suffix = 'Complete', length = 60)\n",
    "\n",
    "a = dp.fetch_raw_data(stock_list_test, '2010-01-01', '2017-12-20')\n",
    "print(a.columns)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example MultiIndex Slicing\n",
    "\n",
    "Ref:\n",
    "- [1] https://pandas.pydata.org/pandas-docs/stable/advanced.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_id_1 = '000001'\n",
    "stock_id_2 = '000002'\n",
    "\n",
    "# to get one stock's data\n",
    "stock_1_data = a.loc[stock_id_1]\n",
    "stock_2_data = a.loc[stock_id_2]\n",
    "print(stock_1_data.shape)\n",
    "print(stock_2_data.shape)\n",
    "print(stock_1_data.describe())\n",
    "print(stock_2_data.describe())\n",
    "\n",
    "# to get all stock's data on a specific date\n",
    "# Don't know yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用举例\n",
    "\n",
    "- 使用了两个DataFrame函数：join()， 和 drop()\n",
    "- 使用 rename()\n",
    "- 使用花括号，curl braces {}, to define a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples. Get base data\n",
    "import tushare as ts\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df1 = ts.get_k_data('000421', start='2011-01-01', end='2017-12-01')\n",
    "df1.set_index('date', inplace=True)\n",
    "print(df1.shape, df1.index)\n",
    "print(df1.columns)\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子， 为股票增加feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add 1st featue\n",
    "df1_propulsion = ttm_propulsion(df1)\n",
    "# a = df1_propulsion.join(df1.drop(['code'], axis=1))\n",
    "a = df1.join(df1_propulsion)\n",
    "\n",
    "# a.rename(columns={'EMA8':'EMA8a', 'EMA21':'EMA21a'}, inplace=True)\n",
    "\n",
    "# add 2nd feature，concatination\n",
    "b = a.join(ttm_squeeze(a))\n",
    "\n",
    "# add 3rd feature, concat. TTM Wave C/B/A\n",
    "d = b.join(ttm_wave(b))\n",
    "\n",
    "print(d.columns)\n",
    "print(d.describe())\n",
    "\n",
    "sd = '2017-05-04'\n",
    "ed = '2017-12-04'\n",
    "print(\"TTM Wave C:\")\n",
    "d.loc[sd:ed][['HIST5', 'MACD6']].plot()\n",
    "plt.show()\n",
    "#print(d.loc['2017-09-10':ed][['close', 'HIST5', 'MACD6', 'SQUEEZE', 'MTMMA']])\n",
    "\n",
    "print(\"TTM Wave B:\")\n",
    "d.loc[sd:ed][['HIST3', 'HIST4']].plot()\n",
    "plt.show()\n",
    "#print(d.loc[sd:ed][['close', 'EMA8', 'EMA21', 'HIST3', 'HIST4']])\n",
    "\n",
    "print(\"TTM Wave A:\")\n",
    "d.loc[sd:ed][['HIST1', 'HIST2']].plot()\n",
    "plt.show()\n",
    "#print(d.loc[sd:ed][['close', 'EMA8', 'EMA21', 'HIST1', 'HIST2']])\n",
    "\n",
    "# add 4th feature, concat. ADX\n",
    "e = d.join(talib_adx(d))\n",
    "print(e.columns)\n",
    "print(\"ADX:\")\n",
    "e.loc[sd:ed]['ADX'].plot()\n",
    "plt.show()\n",
    "e.loc[sd:ed][['close']].plot()\n",
    "plt.show()\n",
    "print(e.loc[sd:ed][['ADX', 'SQUEEZE', 'MTMMA']])\n",
    "\n",
    "# add 5th feature, N-Bar low\n",
    "f = e.join(talib_nbarlow(e, 10))\n",
    "print(f.columns)\n",
    "f.loc[sd:ed][['close', 'LOW10']].plot()\n",
    "plt.show()\n",
    "\n",
    "# add 6th feaure, ATR\n",
    "\n",
    "g = f.join(talib_atr(f))\n",
    "print(g.columns)\n",
    "g.loc[sd:ed][['ATR']].plot()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE\n",
    "# create Series from Dictionary\n",
    "\n",
    "data = {'A':1, 'B':2, 'C':3}\n",
    "y_sample = pd.Series(data)\n",
    "print(y_sample)\n",
    "print(y_sample['A'])\n",
    "\n",
    "data2 = {'A':10, 'B':20, 'C':pd.np.nan}\n",
    "y_sample2 = pd.Series(data2)\n",
    "\n",
    "print(y_sample2)\n",
    "\n",
    "dft = pd.DataFrame()\n",
    "dft = dft.append(y_sample, ignore_index=True)\n",
    "if dft.isnull().values.any():\n",
    "    print('there is NaN')\n",
    "else:\n",
    "    print('there is no NaN')\n",
    "\n",
    "dft = dft.append(y_sample2, ignore_index=True)\n",
    "print(dft)\n",
    "\n",
    "print('append a dataframe')\n",
    "dft = dft.append(dft, ignore_index=True)\n",
    "print(dft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选出符合规则的数据做 训练用例\n",
    "\n",
    "从给定的股票数据，根据 买入规则， 卖出规则，选出相符合的数据序列，用作 训练用例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# generate_samples\n",
    "# Explain:\n",
    "#    - from a given stock_data, generate buy/sell samples\n",
    "#    - currently, using is_squeeze_buy_point() to find buy points\n",
    "#                 using get_sell_point() to find sell points\n",
    "# Input:\n",
    "#    - stock_data: stock_data with features\n",
    "# Output:\n",
    "#    - X_all: all samples' X part, concatenated, in DataFrame\n",
    "#    - Y_all: all samples' Y part, concatenated, in DataFrame\n",
    "#\n",
    "\n",
    "# number of bars to look back to form a sample\n",
    "CONST_LOOKBACK_SAMPLES = 30\n",
    "\n",
    "def generate_samples(stock_data):\n",
    "    # Initialize X_all and Y_all\n",
    "    X_all = pd.DataFrame()\n",
    "    Y_all = pd.DataFrame()\n",
    "\n",
    "    for index, row in stock_data.iterrows():\n",
    "        # is this row a buy-point?\n",
    "        if not is_squeeze_buy_point(stock_data, index):\n",
    "            ## print(index + ' is NOT')\n",
    "            # check next row\n",
    "            continue\n",
    "\n",
    "        # Yes, it is a buy-point.\n",
    "        # Let's check when is the sell-point.\n",
    "        sell_index, sell_reason = get_sell_point(stock_data, index)\n",
    "\n",
    "        # Do we hit a sell point?\n",
    "        if sell_reason == 0: # No, skip it\n",
    "            continue\n",
    "\n",
    "        ## print('=======')\n",
    "        ## print(index + ' is YES')\n",
    "        ## print(sell_index, 'is SELL POINT')\n",
    "        ## print(sell_reason, 'is SELL REASON')\n",
    "        ## print('Buy  @ ', stock_data['close'][index])\n",
    "        ## print('Sell @ ', stock_data['close'][sell_index])\n",
    "        ## print('=======')\n",
    "\n",
    "        # Back fetch N-record\n",
    "        location_of_buy_point = stock_data.index.get_loc(index)\n",
    "        first_location = location_of_buy_point - CONST_LOOKBACK_SAMPLES + 1\n",
    "        if first_location < 0: # there is no enough records to form a valid sample\n",
    "            # skip it\n",
    "            continue\n",
    "\n",
    "        # Slicing. These are totally CONST_LOOKBACK_SAMPLES of records.\n",
    "        x_sample = stock_data.iloc[first_location:(location_of_buy_point + 1)]\n",
    "        # x_sampel Validity check.\n",
    "        if x_sample.isnull().values.any():\n",
    "            # skip it\n",
    "            continue\n",
    "\n",
    "        # create y_sample as a pandas.Series\n",
    "        y_raw_data = {'code': row['code'],\n",
    "                      'buy_date': index,\n",
    "                      'buy_price': stock_data['close'][index],\n",
    "                      'sell_date': sell_index,\n",
    "                      'sell_price': stock_data['close'][sell_index],\n",
    "                      'sell_reason': sell_reason}\n",
    "        y_sample = pd.Series(y_raw_data)\n",
    "        # y_sample validity check\n",
    "        if y_sample.isnull().values.any():\n",
    "            # skip it\n",
    "            continue\n",
    "\n",
    "        # Add N-record to X_all\n",
    "        X_all = X_all.append(x_sample, ignore_index = False)\n",
    "        # Add sell-point information to Y_all.\n",
    "        Y_all = Y_all.append(y_sample, ignore_index = True)\n",
    "\n",
    "        #### END of for loop ####\n",
    "\n",
    "    return X_all, Y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing and Studying\n",
    "# Examples of basic syntax\n",
    "\n",
    "import talib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data preparation\n",
    "# stock_data = stock_1_data\n",
    "sd = '2015-01-30'\n",
    "ed = '2016-01-30'\n",
    "stock_data = g[sd:ed]\n",
    "print(stock_data.columns)\n",
    "print(type(stock_data.index))\n",
    "\n",
    "cur_date = '2015-10-01'\n",
    "remaining_s_data = stock_data[cur_date:]\n",
    "\n",
    "print('first index in remaing_s_data', remaining_s_data.index[0])\n",
    "print('it\\'s location in stock_data', stock_data.index.get_loc(remaining_s_data.index[0]))\n",
    "\n",
    "buy_point_loc = stock_data.index.get_loc(remaining_s_data.index[0])\n",
    "first_loc = buy_point_loc - 5 + 1\n",
    "\n",
    "# slicing\n",
    "sample_x_test = stock_data.iloc[first_loc:(buy_point_loc+1)]\n",
    "\n",
    "print(sample_x_test)\n",
    "\n",
    "print('remaining data\\'s index')\n",
    "print(remaining_s_data.index)\n",
    "\n",
    "# 10 days low\n",
    "N_BAR_LOWEST = 10\n",
    "nbar_lowest_col_name = 'LOW' + str(N_BAR_LOWEST)\n",
    "stock_data[nbar_lowest_col_name] = talib.MIN(stock_data['low'].values, timeperiod = N_BAR_LOWEST)\n",
    "stock_data[nbar_lowest_col_name] = stock_data[nbar_lowest_col_name].shift(1)\n",
    "\n",
    "stock_data[['close', nbar_lowest_col_name]].plot()\n",
    "plt.show()\n",
    "print(stock_data[['low', 'close', nbar_lowest_col_name]])\n",
    "\n",
    "print('===========')\n",
    "print('===========')\n",
    "print('===========')\n",
    "print('===========')\n",
    "\n",
    "X_all, Y_all = generate_samples(stock_data)\n",
    "print(X_all.shape, Y_all.shape)\n",
    "print(Y_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### 定义 SQUEEZE 选股规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# is_squeeze_buy_point\n",
    "# Explain:\n",
    "#    - Is the current index'ed bar a squeeze buy-point?\n",
    "# Input:\n",
    "#    - stock_data: stock_data with features 'SQUEEZE', TTM Wave C/B/A, and ADX ready\n",
    "#    - index: current index\n",
    "# Output:\n",
    "#    - Return: boolean, True or False\n",
    "#\n",
    "\n",
    "# define squeeze CONST\n",
    "CONST_SQUEEZE_RELEASED = -1\n",
    "CONST_SQUEEZE_ONGOING = 1\n",
    "\n",
    "def is_squeeze_buy_point(stock_data, index):\n",
    "    # FEATUREs Required: \n",
    "    #      - 'SQUEEZE', 'HIST5', and 'MACD6'\n",
    "    # RULE:\n",
    "    #      - a) TTM Wave C, ie. 'HIST5' and 'MACD6', must be greater than '0'. Then,\n",
    "    #      - b) 'SQUEEZE' should be either ongoing, or on the first bar of releasing.\n",
    "    # \n",
    "    ret = False\n",
    "    \n",
    "    # test TTM Wave C > 0\n",
    "    if (stock_data.loc[index, 'HIST5'] <= 0) or (stock_data.loc[index, 'MACD6'] <= 0):\n",
    "        ret = False\n",
    "        return ret\n",
    "    \n",
    "    # test whether 'SQUEEZE' is on-going\n",
    "    if (stock_data.loc[index, 'SQUEEZE'] == CONST_SQUEEZE_ONGOING):\n",
    "        ret = True\n",
    "        return ret\n",
    "    \n",
    "    # test whether on the first bar of 'SQUEEZE' release\n",
    "    if (stock_data.loc[index, 'SQUEEZE'] == CONST_SQUEEZE_RELEASED):\n",
    "        # check previous bar is 'SQUEEZE' ongoing?\n",
    "        if (stock_data['SQUEEZE'].shift(1)[index] == CONST_SQUEEZE_ONGOING):\n",
    "            ret = True\n",
    "            return ret\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义卖出规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_sell_point\n",
    "# Explain:\n",
    "#    - Find the sell point\n",
    "# Input:\n",
    "#    - stock_data: stock data with features\n",
    "#    - buy_index: the bar where we start the trade\n",
    "#    - multi_atr: multiple of ATR as stop loss at\n",
    "#    - n_low: close breaks n_low bar's low\n",
    "# Output:\n",
    "#    - sell_index: index of the sell point, or\n",
    "#                  0 if cannot find sell point before the end\n",
    "#    - sell_reason: 1 for stop loss, or 2 for N-bar low breakthrough.\n",
    "#                  0 if not reached any sell point.\n",
    "#\n",
    "\n",
    "CONST_SELL_REASON_STOP_LOSS = 1\n",
    "CONST_SELL_REASON_N_LOW =2\n",
    "\n",
    "def get_sell_point(stock_data, buy_index, multi_atr = 2, n_low = 10):\n",
    "    # FEATUREs Required: \n",
    "    #      - 'close', 'ATR'\n",
    "    # RULE: check each bar after buy_index\n",
    "    #      - a) close price is lower than (multi_atr * ATR)\n",
    "    #      - b) close lower than prvious n_low bars' low\n",
    "    # \n",
    "    sell_index = 0\n",
    "    sell_reason = 0\n",
    "    \n",
    "    # where we start the trade\n",
    "    buy_price = stock_data['close'][buy_index]\n",
    "    stop_price = buy_price - stock_data['ATR'][buy_index] * multi_atr\n",
    "    #print('buy_price: ', buy_price)\n",
    "    #print('stop_price: ', stop_price)\n",
    "    \n",
    "    remaining_stock_data = stock_data[buy_index:]\n",
    "    n_low_col_name = 'LOW' + str(n_low)\n",
    "            \n",
    "    # check when to sell\n",
    "    for index, row in remaining_stock_data.iterrows():\n",
    "        if row['close'] <= stop_price: # down-break stop_price\n",
    "            # print('today close is lowet than stop_price')\n",
    "            # print(row['close'])\n",
    "            # print(stop_price)\n",
    "            sell_index = index\n",
    "            sell_reason = CONST_SELL_REASON_STOP_LOSS\n",
    "            break\n",
    "        \n",
    "        # close at a price lower than previous n_low bars' low\n",
    "        if row['close'] <= row[n_low_col_name]:\n",
    "            # print(row[['close', col_name]])\n",
    "            sell_index = index\n",
    "            sell_reason = CONST_SELL_REASON_N_LOW\n",
    "            break\n",
    "    \n",
    "    return sell_index, sell_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input:\n",
    "#    - stock_data: \n",
    "\n",
    "# iterrate each row of stock_data\n",
    "for index, row in stock_data.iterrows():\n",
    "    print(index)\n",
    "    # is this row a buy-point?\n",
    "    if not is_squeeze_buy_point(stock_data, index, row):\n",
    "        # check next row\n",
    "        continue\n",
    "        \n",
    "    # Yes, this is a buying-point\n",
    "    # Find the sell-point\n",
    "    index_sell = get_sell_point(stock_data, buy_index)\n",
    "    \n",
    "    # do we find a valid sell-point? Maybe before our sequence ends, it doesn't reach any sell-point.\n",
    "    if NOT valid(index_sell):\n",
    "        # check next row\n",
    "        continue\n",
    "\n",
    "    # Back fetch N-record\n",
    "    # check N-record valid or not\n",
    "    # Add N-record to X_all\n",
    "    # Add sell-point information to Y_all.\n",
    "    \n",
    "    #### END ####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
